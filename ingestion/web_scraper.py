import requests
from bs4 import BeautifulSoup
import sys
import os
import uuid

# Dodajemo putanju do projekta da vidimo naÅ¡e module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from embeddings.embedding_client import EmbeddingClient
from vector_db.chroma_client import ChromaDBClient

class GrantScraper:
    def __init__(self):
        self.embedder = EmbeddingClient()
        self.db = ChromaDBClient()
        # "LaÅ¾iramo" da smo pravi browser da nas ne blokiraju
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def scrape_and_store(self, url: str):
        print(f"\nğŸŒ Povezujem se na: {url} ...")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code != 200:
                print(f"âŒ GreÅ¡ka pri pristupu stranici: Status {response.status_code}")
                return

            # Parsiranje HTML-a
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ÄŒiÅ¡Ä‡enje: Uzimamo naslov i tekstove paragrafa
            title = soup.title.string if soup.title else "Nepoznat naslov"
            
            # Trik: Uzimamo sve <p> (paragrafe) i <li> (liste) jer tu bude tekst konkursa
            text_blocks = [p.get_text().strip() for p in soup.find_all(['p', 'li', 'h1', 'h2', 'h3']) if len(p.get_text().strip()) > 50]
            
            # Spajamo u veÄ‡e cjeline (chunks) da ne embedamo svaku reÄenicu posebno
            full_text = "\n".join(text_blocks)
            
            # Ako je tekst predug, uzimamo prvih 2000 karaktera (radi Å¡tednje i brzine)
            # U naprednoj verziji bi ovo sjeckali na dijelove (chunking)
            final_content = f"IZVOR: {title} ({url})\n\nSADRÅ½AJ:\n{full_text[:4000]}"
            
            if len(full_text) < 100:
                print("âš ï¸ Upozorenje: PronaÄ‘eno premalo teksta. MoÅ¾da stranica koristi JavaScript zaÅ¡titu.")
                return

            print(f"ğŸ“„ UspjeÅ¡no skinuto! Naslov: {title}")
            print(f"ğŸ“Š VeliÄina teksta: {len(final_content)} karaktera")

            # --- RAG PROCES ---
            # 1. Priprema podataka
            doc_id = str(uuid.uuid4())
            metadata = {"source": url, "title": title, "type": "scraped_web"}
            
            print("ğŸ”„ Generisanje vektora (Geminija)...")
            embedding = self.embedder.embed_single(final_content)
            
            print("ğŸ’¾ Spremanje u ChromaDB...")
            self.db.add(
                documents=[final_content],
                metadatas=[metadata],
                ids=[doc_id],
                embeddings=[embedding] # embed_single vraÄ‡a listu floatova, a add traÅ¾i listu listi
            )
            
            print("âœ… USJPJEÅ NO! Ovaj link je sada dio 'mozga' tvog AI agenta.")

        except Exception as e:
            print(f"âŒ KritiÄna greÅ¡ka: {e}")

if __name__ == "__main__":
    scraper = GrantScraper()
    print("--- ğŸ•·ï¸ AI GRANT SCRAPER ---")
    print("Unesi URL stranice sa koje Å¾eliÅ¡ 'pokupiti' znanje.")
    target_url = input("URL >> ").strip()
    
    if target_url:
        scraper.scrape_and_store(target_url)
    else:
        print("Nisi unio URL.")
